{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект для «Викишоп»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from langdetect import detect\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import (train_test_split, \n",
    "                                     RandomizedSearchCV)\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Импортирую библиотеку для отображения статус-бара\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE = np.random.RandomState(12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Получение данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Прочитали данные с диска\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    data = pd.read_csv('C:/0/d/toxic_comments.csv')\n",
    "    print('Прочитали данные с диска')\n",
    "except:\n",
    "    data = pd.read_csv('/datasets/toxic_comments.csv')\n",
    "    print('Прочитали данные в сети')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ознакомимся с данными"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  toxic\n",
       "0           0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1           1  D'aww! He matches this background colour I'm s...      0\n",
       "2           2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3           3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4           4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159292 entries, 0 to 159291\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   Unnamed: 0  159292 non-null  int64 \n",
      " 1   text        159292 non-null  object\n",
      " 2   toxic       159292 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Установим индекс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns = {'Unnamed: 0':'id'}, inplace = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sort_values(by='id',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Значения индекса уникальны\n"
     ]
    }
   ],
   "source": [
    "if len(data.id) != len(data.id.unique()):\n",
    "    print('Значения индекса повторяются')\n",
    "else: print('Значения индекса уникальны')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.set_index('id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переведем все тексты в нижний регистр (поскольку словарь стоп-слов работает с нижним регистром)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>explanation\\nwhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d'aww! he matches this background colour i'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hey man, i'm really not trying to edit war. it...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nmore\\ni can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>you, sir, are my hero. any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  toxic\n",
       "id                                                          \n",
       "0   explanation\\nwhy the edits made under my usern...      0\n",
       "1   d'aww! he matches this background colour i'm s...      0\n",
       "2   hey man, i'm really not trying to edit war. it...      0\n",
       "3   \"\\nmore\\ni can't make any real suggestions on ...      0\n",
       "4   you, sir, are my hero. any chance you remember...      0"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подвыборка\n",
    "Поскольку датасет большой для отработки алгоритма создам функцию для подвыборки, для отработки методов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample(data, fraction):\n",
    "    \"\"\"\n",
    "    Получение случайной подвыборки из исходной выборки.\n",
    "    :param data: исходная выборка\n",
    "    :param fraction: доля (от 0 до 1), которую отбираем из исходной выборки\n",
    "    :return: подвыборка\n",
    "    \"\"\"\n",
    "    return data.sample(frac=fraction, random_state=STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее беру всю выборку целиком."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = get_sample(data, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проверка языков датасета"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценим долю языков по датасету. Функцию ниже закомментировал. Она рабочая, и понадобилась только один раз для проверки данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def detect_languages(data):\n",
    "#     num_english, num_russian, num_other = 0, 0, 0\n",
    "#     for text in tqdm(data['text']):\n",
    "#         try:\n",
    "#             lang = detect(text)\n",
    "#         except:\n",
    "#             lang = 'unknown'\n",
    "#         if lang == 'en':\n",
    "#             num_english += 1\n",
    "#         elif lang == 'ru':\n",
    "#             num_russian += 1\n",
    "#         else:\n",
    "#             num_other += 1\n",
    "#     total = num_english + num_russian + num_other\n",
    "#     print(f\"English: {num_english/total:.2%}, Russian: {num_russian/total:.2%}, Other: {num_other/total:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Примерное время работы данной функции на моем компьютере для полного датасета составило 15 минут. Также сам результат функции нигде не используется, поэтому, я закомментировал блок ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect_languages(data)\n",
    "# detect_languages(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Доля английского на всем датасете составляет почти 97%. Поэтому для лемматизации буду использовать nltk без русского словаря"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Токенизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "загрузим набор данных, содержащий предобученные модели токенизации для разных языков\n",
    "\"\"\"\n",
    "nltk.download('punkt')\n",
    "\n",
    "def tokenize_text(text):\n",
    "    \"\"\"\n",
    "    Токенизация текста на отдельные слова.\n",
    "    :param text: текст для токенизации\n",
    "    :return: список токенов\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159292/159292 [01:29<00:00, 1771.85it/s]\n"
     ]
    }
   ],
   "source": [
    "# токенизируем текст. время на весь датасет около 2 минут\n",
    "sample['tokens'] = [tokenize_text(text) for text in tqdm(sample['text'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "109583    [expert, categorizers, why, is, there, no, men...\n",
       "105077                      [``, noise, fart, *, talk., ``]\n",
       "82244     [an, indefinite, block, is, appropriate, ,, ev...\n",
       "18740     [i, do, n't, understand, why, we, have, a, scr...\n",
       "128310    [hello, !, some, of, the, people, ,, places, o...\n",
       "                                ...                        \n",
       "110090    [hahaha, ., ), i, dont, live, in, a, lie, like...\n",
       "85493             [march, 2006, –, march, 2006, ], ], |, }]\n",
       "133387    [``, agreed, ., we, really, should, try, to, s...\n",
       "130469    [``, umm, killer, do, you, not, like, that, he...\n",
       "77361     [bradford, city, i, am, removing, unreferanced...\n",
       "Name: tokens, Length: 159292, dtype: object"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['tokens']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Удаление нетекстовых символов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nonletters(tokens):\n",
    "    \"\"\"\n",
    "    Удаление символов, кроме букв, из списка токенов.\n",
    "    :param tokens: список токенов для обработки\n",
    "    :return: список токенов с удаленными символами, кроме букв\n",
    "    \"\"\"\n",
    "    pattern = r'[^a-zA-Zа-яА-Я\\s]'\n",
    "    # удаление символов в каждом токене\n",
    "    filtered_tokens = [re.sub(pattern, '', token) for token in tokens]\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159292/159292 [00:12<00:00, 12563.84it/s]\n"
     ]
    }
   ],
   "source": [
    "# время - около 15 сек\n",
    "sample['tokens'] = [remove_nonletters(text) for text in tqdm(sample['tokens'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "109583    [expert, categorizers, why, is, there, no, men...\n",
       "105077                            [, noise, fart, , talk, ]\n",
       "82244     [an, indefinite, block, is, appropriate, , eve...\n",
       "18740     [i, do, nt, understand, why, we, have, a, scre...\n",
       "128310    [hello, , some, of, the, people, , places, or,...\n",
       "                                ...                        \n",
       "110090    [hahaha, , , i, dont, live, in, a, lie, like, ...\n",
       "85493                          [march, , , march, , , , , ]\n",
       "133387    [, agreed, , we, really, should, try, to, stic...\n",
       "130469    [, umm, killer, do, you, not, like, that, he, ...\n",
       "77361     [bradford, city, i, am, removing, unreferanced...\n",
       "Name: tokens, Length: 159292, dtype: object"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['tokens']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Лемматизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузка WPOS-теггера и словаря\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "# Загрузка английского словаря с лексикой\n",
    "nltk.download('wordnet')\n",
    "# Загрузка русского словаря с лексикой\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(tag):\n",
    "    \"\"\"\n",
    "    Преобразование POS-тегов из формата, используемого в nltk.pos_tag(), в формат,\n",
    "    используемый в WordNetLemmatizer.\n",
    "    :param tag: POS-тег из nltk.pos_tag()\n",
    "    :return: POS-тег для WordNetLemmatizer\n",
    "    \"\"\"\n",
    "    if tag.startswith('J'):\n",
    "        return 'a'  # прилагательное\n",
    "    elif tag.startswith('V'):\n",
    "        return 'v'  # глагол\n",
    "    elif tag.startswith('N'):\n",
    "        return 'n'  # существительное\n",
    "    elif tag.startswith('R'):\n",
    "        return 'r'  # наречие\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_with_pos(tokens):\n",
    "    \"\"\"\n",
    "    Лемматизация токенов с указанием части речи.\n",
    "    :param tokens: список токенов\n",
    "    :return: список лемматизированных токенов\n",
    "    \"\"\"\n",
    "    # Получение POS-тегов для каждого токена\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    # Преобразование POS-тегов к формату, который используется в WordNetLemmatizer\n",
    "    pos_tags = [(tag[0], get_wordnet_pos(tag[1])) for tag in pos_tags]\n",
    "    # Лемматизация токенов с учетом их части речи\n",
    "    lemmatized_tokens = []\n",
    "    for tag in pos_tags:\n",
    "        if tag[1] is not None:\n",
    "            lemma = lemmatizer.lemmatize(tag[0], tag[1])\n",
    "            lemmatized_tokens.append(lemma)\n",
    "    return lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159292/159292 [13:46<00:00, 192.64it/s]\n"
     ]
    }
   ],
   "source": [
    "# время на всем датафрейме - около 14 минут\n",
    "sample['lemms'] = [lemmatize_with_pos(text) for text in tqdm(sample['tokens'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравним, сколько по времени выполняется apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "# можно не запускать - это тест - для сравнения скорости\n",
    "# %time\n",
    "# start = time.time()\n",
    "# sample['lemms_prog_applay'] = sample['tokens'].apply(lemmatize_with_pos)\n",
    "# end = time.time()\n",
    "# l_prog_apply_time = end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.7402152578036"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# l_prog_apply_time/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemms</th>\n",
       "      <th>lemms_prog_applay</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>109583</th>\n",
       "      <td>expert categorizers  \\n\\nwhy is there no menti...</td>\n",
       "      <td>0</td>\n",
       "      <td>[expert, categorizers, why, is, there, no, men...</td>\n",
       "      <td>[expert, categorizers, be, there, mention, fac...</td>\n",
       "      <td>[expert, categorizers, be, there, mention, fac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105077</th>\n",
       "      <td>\"\\n\\n noise \\n\\nfart*  talk. \"</td>\n",
       "      <td>1</td>\n",
       "      <td>[, noise, fart, , talk, ]</td>\n",
       "      <td>[, noise, fart, , talk, ]</td>\n",
       "      <td>[, noise, fart, , talk, ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82244</th>\n",
       "      <td>an indefinite block is appropriate, even for a...</td>\n",
       "      <td>0</td>\n",
       "      <td>[an, indefinite, block, is, appropriate, , eve...</td>\n",
       "      <td>[indefinite, block, be, appropriate, , even, m...</td>\n",
       "      <td>[indefinite, block, be, appropriate, , even, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18740</th>\n",
       "      <td>i don't understand why we have a screenshot of...</td>\n",
       "      <td>0</td>\n",
       "      <td>[i, do, nt, understand, why, we, have, a, scre...</td>\n",
       "      <td>[i, do, understand, have, screenshot, ap, s, g...</td>\n",
       "      <td>[i, do, understand, have, screenshot, ap, s, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128310</th>\n",
       "      <td>hello! some of the people, places or things yo...</td>\n",
       "      <td>0</td>\n",
       "      <td>[hello, , some, of, the, people, , places, or,...</td>\n",
       "      <td>[hello, , people, , place, thing, have, write,...</td>\n",
       "      <td>[hello, , people, , place, thing, have, write,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110090</th>\n",
       "      <td>hahaha. ) i dont live in a lie like you and do...</td>\n",
       "      <td>1</td>\n",
       "      <td>[hahaha, , , i, dont, live, in, a, lie, like, ...</td>\n",
       "      <td>[hahaha, , , i, dont, live, lie, dont, deny, t...</td>\n",
       "      <td>[hahaha, , , i, dont, live, lie, dont, deny, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85493</th>\n",
       "      <td>march 2006 – march 2006]]\\n \\n\\n|}</td>\n",
       "      <td>0</td>\n",
       "      <td>[march, , , march, , , , , ]</td>\n",
       "      <td>[march, , , march, , , , , ]</td>\n",
       "      <td>[march, , , march, , , , , ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133387</th>\n",
       "      <td>\"\\n\\nagreed.  we really should try to stick to...</td>\n",
       "      <td>0</td>\n",
       "      <td>[, agreed, , we, really, should, try, to, stic...</td>\n",
       "      <td>[, agree, really, try, stick, subject, article...</td>\n",
       "      <td>[, agree, really, try, stick, subject, article...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130469</th>\n",
       "      <td>\"\\n\\n umm killer \\n\\ndo you not like that he c...</td>\n",
       "      <td>0</td>\n",
       "      <td>[, umm, killer, do, you, not, like, that, he, ...</td>\n",
       "      <td>[, umm, killer, do, not, copy, whole, userpage...</td>\n",
       "      <td>[, umm, killer, do, not, copy, whole, userpage...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77361</th>\n",
       "      <td>bradford city \\n\\ni am removing unreferanced c...</td>\n",
       "      <td>0</td>\n",
       "      <td>[bradford, city, i, am, removing, unreferanced...</td>\n",
       "      <td>[bradford, city, i, be, remove, unreferanced, ...</td>\n",
       "      <td>[bradford, city, i, be, remove, unreferanced, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159292 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  toxic  \\\n",
       "id                                                                 \n",
       "109583  expert categorizers  \\n\\nwhy is there no menti...      0   \n",
       "105077                     \"\\n\\n noise \\n\\nfart*  talk. \"      1   \n",
       "82244   an indefinite block is appropriate, even for a...      0   \n",
       "18740   i don't understand why we have a screenshot of...      0   \n",
       "128310  hello! some of the people, places or things yo...      0   \n",
       "...                                                   ...    ...   \n",
       "110090  hahaha. ) i dont live in a lie like you and do...      1   \n",
       "85493                  march 2006 – march 2006]]\\n \\n\\n|}      0   \n",
       "133387  \"\\n\\nagreed.  we really should try to stick to...      0   \n",
       "130469  \"\\n\\n umm killer \\n\\ndo you not like that he c...      0   \n",
       "77361   bradford city \\n\\ni am removing unreferanced c...      0   \n",
       "\n",
       "                                                   tokens  \\\n",
       "id                                                          \n",
       "109583  [expert, categorizers, why, is, there, no, men...   \n",
       "105077                          [, noise, fart, , talk, ]   \n",
       "82244   [an, indefinite, block, is, appropriate, , eve...   \n",
       "18740   [i, do, nt, understand, why, we, have, a, scre...   \n",
       "128310  [hello, , some, of, the, people, , places, or,...   \n",
       "...                                                   ...   \n",
       "110090  [hahaha, , , i, dont, live, in, a, lie, like, ...   \n",
       "85493                        [march, , , march, , , , , ]   \n",
       "133387  [, agreed, , we, really, should, try, to, stic...   \n",
       "130469  [, umm, killer, do, you, not, like, that, he, ...   \n",
       "77361   [bradford, city, i, am, removing, unreferanced...   \n",
       "\n",
       "                                                    lemms  \\\n",
       "id                                                          \n",
       "109583  [expert, categorizers, be, there, mention, fac...   \n",
       "105077                          [, noise, fart, , talk, ]   \n",
       "82244   [indefinite, block, be, appropriate, , even, m...   \n",
       "18740   [i, do, understand, have, screenshot, ap, s, g...   \n",
       "128310  [hello, , people, , place, thing, have, write,...   \n",
       "...                                                   ...   \n",
       "110090  [hahaha, , , i, dont, live, lie, dont, deny, t...   \n",
       "85493                        [march, , , march, , , , , ]   \n",
       "133387  [, agree, really, try, stick, subject, article...   \n",
       "130469  [, umm, killer, do, not, copy, whole, userpage...   \n",
       "77361   [bradford, city, i, be, remove, unreferanced, ...   \n",
       "\n",
       "                                        lemms_prog_applay  \n",
       "id                                                         \n",
       "109583  [expert, categorizers, be, there, mention, fac...  \n",
       "105077                          [, noise, fart, , talk, ]  \n",
       "82244   [indefinite, block, be, appropriate, , even, m...  \n",
       "18740   [i, do, understand, have, screenshot, ap, s, g...  \n",
       "128310  [hello, , people, , place, thing, have, write,...  \n",
       "...                                                   ...  \n",
       "110090  [hahaha, , , i, dont, live, lie, dont, deny, t...  \n",
       "85493                        [march, , , march, , , , , ]  \n",
       "133387  [, agree, really, try, stick, subject, article...  \n",
       "130469  [, umm, killer, do, not, copy, whole, userpage...  \n",
       "77361   [bradford, city, i, be, remove, unreferanced, ...  \n",
       "\n",
       "[159292 rows x 5 columns]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Удаление стоп-слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# загрузим словарь стоп-слов\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens):\n",
    "    \"\"\"\n",
    "    Удаление стоп-слов из списка токенов.\n",
    "    :param tokens: список токенов для обработки\n",
    "    :return: список токенов с удаленными стоп-словами\n",
    "    \"\"\"\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159292/159292 [00:01<00:00, 112998.47it/s]\n"
     ]
    }
   ],
   "source": [
    "sample['clear'] = [remove_stopwords(text) for text in tqdm(sample['lemms'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Объединение токенов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_tokens(tokens):\n",
    "    return ' '.join(tokens) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159292/159292 [00:00<00:00, 377433.64it/s]\n"
     ]
    }
   ],
   "source": [
    "sample['clear'] = [join_tokens(text) for text in tqdm(sample['clear'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemms</th>\n",
       "      <th>lemms_prog_applay</th>\n",
       "      <th>clear</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>109583</th>\n",
       "      <td>expert categorizers  \\n\\nwhy is there no menti...</td>\n",
       "      <td>0</td>\n",
       "      <td>[expert, categorizers, why, is, there, no, men...</td>\n",
       "      <td>[expert, categorizers, be, there, mention, fac...</td>\n",
       "      <td>[expert, categorizers, be, there, mention, fac...</td>\n",
       "      <td>expert categorizers mention fact nazis particu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105077</th>\n",
       "      <td>\"\\n\\n noise \\n\\nfart*  talk. \"</td>\n",
       "      <td>1</td>\n",
       "      <td>[, noise, fart, , talk, ]</td>\n",
       "      <td>[, noise, fart, , talk, ]</td>\n",
       "      <td>[, noise, fart, , talk, ]</td>\n",
       "      <td>noise fart  talk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82244</th>\n",
       "      <td>an indefinite block is appropriate, even for a...</td>\n",
       "      <td>0</td>\n",
       "      <td>[an, indefinite, block, is, appropriate, , eve...</td>\n",
       "      <td>[indefinite, block, be, appropriate, , even, m...</td>\n",
       "      <td>[indefinite, block, be, appropriate, , even, m...</td>\n",
       "      <td>indefinite block appropriate  even minor infra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18740</th>\n",
       "      <td>i don't understand why we have a screenshot of...</td>\n",
       "      <td>0</td>\n",
       "      <td>[i, do, nt, understand, why, we, have, a, scre...</td>\n",
       "      <td>[i, do, understand, have, screenshot, ap, s, g...</td>\n",
       "      <td>[i, do, understand, have, screenshot, ap, s, g...</td>\n",
       "      <td>understand screenshot ap gui ub  someone remedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128310</th>\n",
       "      <td>hello! some of the people, places or things yo...</td>\n",
       "      <td>0</td>\n",
       "      <td>[hello, , some, of, the, people, , places, or,...</td>\n",
       "      <td>[hello, , people, , place, thing, have, write,...</td>\n",
       "      <td>[hello, , people, , place, thing, have, write,...</td>\n",
       "      <td>hello  people  place thing write article tryfo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  toxic  \\\n",
       "id                                                                 \n",
       "109583  expert categorizers  \\n\\nwhy is there no menti...      0   \n",
       "105077                     \"\\n\\n noise \\n\\nfart*  talk. \"      1   \n",
       "82244   an indefinite block is appropriate, even for a...      0   \n",
       "18740   i don't understand why we have a screenshot of...      0   \n",
       "128310  hello! some of the people, places or things yo...      0   \n",
       "\n",
       "                                                   tokens  \\\n",
       "id                                                          \n",
       "109583  [expert, categorizers, why, is, there, no, men...   \n",
       "105077                          [, noise, fart, , talk, ]   \n",
       "82244   [an, indefinite, block, is, appropriate, , eve...   \n",
       "18740   [i, do, nt, understand, why, we, have, a, scre...   \n",
       "128310  [hello, , some, of, the, people, , places, or,...   \n",
       "\n",
       "                                                    lemms  \\\n",
       "id                                                          \n",
       "109583  [expert, categorizers, be, there, mention, fac...   \n",
       "105077                          [, noise, fart, , talk, ]   \n",
       "82244   [indefinite, block, be, appropriate, , even, m...   \n",
       "18740   [i, do, understand, have, screenshot, ap, s, g...   \n",
       "128310  [hello, , people, , place, thing, have, write,...   \n",
       "\n",
       "                                        lemms_prog_applay  \\\n",
       "id                                                          \n",
       "109583  [expert, categorizers, be, there, mention, fac...   \n",
       "105077                          [, noise, fart, , talk, ]   \n",
       "82244   [indefinite, block, be, appropriate, , even, m...   \n",
       "18740   [i, do, understand, have, screenshot, ap, s, g...   \n",
       "128310  [hello, , people, , place, thing, have, write,...   \n",
       "\n",
       "                                                    clear  \n",
       "id                                                         \n",
       "109583  expert categorizers mention fact nazis particu...  \n",
       "105077                                  noise fart  talk   \n",
       "82244   indefinite block appropriate  even minor infra...  \n",
       "18740    understand screenshot ap gui ub  someone remedy   \n",
       "128310  hello  people  place thing write article tryfo...  "
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Векторизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку обучать модели буду с помощью рандомсерча, векторизацию буду делать для всех фолдов внутри рандомсерча. Поэтому пока просто разделю выборки на обучающую и тестовую"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = \\\n",
    "train_test_split(sample['clear'], sample['toxic'], test_size=0.2, random_state=STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Буду обучать 2 модели. Создам датафрейм для записи итоговых данных по моделям"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['LogReg','Light GBM']\n",
    "model_results = pd.DataFrame(columns=['f1_train','time_train','f1_test','time_test'],\n",
    "                             index=models,dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создам функцию для рандомного поиска гиперпараметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_random_cv(model,grid,CV,score,features,target):\n",
    "    # Определяем пайплайн\n",
    "    pipeline = Pipeline([\n",
    "        ('vectorizer', TfidfVectorizer()),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    \n",
    "    random_cv = RandomizedSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_distributions=grid,\n",
    "        cv=CV, \n",
    "        scoring = score,\n",
    "        n_jobs = -1,\n",
    "        random_state=STATE)\n",
    "    random_cv.fit(features, target)\n",
    "    return random_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# для всех моделей\n",
    "scoring = 'f1'\n",
    "CV = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Логистическая регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определяем модель\n",
    "# model_logr = LogisticRegression(class_weight='balanced')\n",
    "model_logr = LogisticRegression(random_state=STATE)\n",
    "# определим пустую сетку\n",
    "# grid = {}\n",
    "# Определяем диапазон параметров\n",
    "grid = {\n",
    "    'model__penalty': ['l1', 'l2'], \n",
    "    'model__C': [0.01, 0.1, 1, 10, 100, 1000],\n",
    "#     'model__fit_intercept': [True, False],\n",
    "#     'model__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "#     'model__max_iter': [50, 100, 200, 500, 1000],\n",
    "#     'vectorizer__max_df': [0.8, 0.9],\n",
    "#     'vectorizer__min_df': [0.01, 0.05],\n",
    "#     'vectorizer__ngram_range': [(1, 1), (1, 2), (2, 2)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [02:04<00:00, 124.76s/it]\n"
     ]
    }
   ],
   "source": [
    "# Подбираем лучшие гиперпараметры - расчет на полном датасете с пустой сеткой ГП- около 30 сек\n",
    "# расчет на полном датасете для сетки ГП\n",
    "#         {'penalty': ['l1', 'l2'],'C': [0.01, 0.1, 1, 10, 100, 1000]} - 2 минуты\n",
    "for i in tqdm(range(1)): # применил цикл с одной итерацией для запуска статус бара tqdm\n",
    "    random_cv_logr = f_random_cv(model_logr,grid,CV,scoring,X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params:  {'model__penalty': 'l2', 'model__C': 10}\n",
      "Best score:  0.7672320284571663\n"
     ]
    }
   ],
   "source": [
    "# лучшие гиперпараметры и значение метрики\n",
    "print(\"Best params: \", random_cv_logr.best_params_)\n",
    "print(\"Best score: \", random_cv_logr.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_fit_time = random_cv_logr.cv_results_['mean_fit_time']\n",
    "total_time = np.sum(mean_fit_time)\n",
    "\n",
    "model_results.loc['LogReg',['f1_train','time_train','f1_test','time_test']]=\\\n",
    "[random_cv_logr.best_score_,total_time,'n/a','n/a']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Light GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# инициализируем модель\n",
    "model_lgb = lgb.LGBMClassifier(random_state=STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# установим сетку гиперпараметров\n",
    "# на пустой сетке\n",
    "# grid = {}\n",
    "grid = {\n",
    "    'model__num_leaves': [10, 20, 30, 40, 50]\n",
    "    ,'model__learning_rate': [0.01,0.05, 0.1, 0.5]\n",
    "    ,'model__max_depth': [-1 , 5, 10, 20]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [10:14<00:00, 614.90s/it]\n"
     ]
    }
   ],
   "source": [
    "# Обучим модель полном датасете\n",
    "# время расчета на пустой сетке - 2 менуты\n",
    "# время расчета на непустой сетке- 10 минут\n",
    "for i in tqdm(range(1)): # применил цикл с одной итерацией для запуска статус бара tqdm\n",
    "    random_cv_lgbm = f_random_cv(model_lgb,grid,CV,scoring,X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params:  {'model__num_leaves': 40, 'model__max_depth': 20, 'model__learning_rate': 0.5}\n",
      "Best score:  0.7564948961711593\n"
     ]
    }
   ],
   "source": [
    "# Выводим лучшие гиперпараметры и значение метрики\n",
    "print(\"Best params: \", random_cv_lgbm.best_params_)\n",
    "print(\"Best score: \", random_cv_lgbm.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_fit_time = random_cv_lgbm.cv_results_['mean_fit_time']\n",
    "total_time = np.sum(mean_fit_time)\n",
    "\n",
    "model_results.loc['Light GBM',['f1_train','time_train','f1_test','time_test']]=\\\n",
    "[random_cv_lgbm.best_score_,total_time,'n/a','n/a']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тестирование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучшей моделью является логистическая регрессия. Тем не менее выведу результаты тестирования для двух моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_logr = random_cv_logr.best_estimator_\n",
    "start = time.time()\n",
    "pred_logr = best_logr.predict(X_test)\n",
    "end = time.time()\n",
    "best_logr_pred_time = end-start\n",
    "best_logr_f1 = f1_score(y_test, pred_logr)\n",
    "model_results.loc['LogReg',['f1_test','time_test']]=\\\n",
    "[round(best_logr_f1,5),round(best_logr_pred_time,6)]\n",
    "# model_results.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1_train</th>\n",
       "      <th>time_train</th>\n",
       "      <th>f1_test</th>\n",
       "      <th>time_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LogReg</th>\n",
       "      <td>0.76723</td>\n",
       "      <td>98.32813</td>\n",
       "      <td>0.77199</td>\n",
       "      <td>1.05985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Light GBM</th>\n",
       "      <td>0.75649</td>\n",
       "      <td>554.22105</td>\n",
       "      <td>0.76399</td>\n",
       "      <td>1.284271</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           f1_train  time_train  f1_test time_test\n",
       "LogReg      0.76723    98.32813  0.77199   1.05985\n",
       "Light GBM   0.75649   554.22105  0.76399  1.284271"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_lgbm = random_cv_lgbm.best_estimator_\n",
    "start = time.time()\n",
    "pred_lgbm = best_lgbm.predict(X_test)\n",
    "end = time.time()\n",
    "best_lgbm_pred_time = end-start\n",
    "best_lgbm_f1 = f1_score(y_test, pred_lgbm)\n",
    "model_results.loc['Light GBM',['f1_test','time_test']]=\\\n",
    "[round(best_lgbm_f1,5),round(best_lgbm_pred_time,6)]\n",
    "model_results.round(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Логистическая регрессия оказалась наиболее эффективной моделью для данной задачи, но необходимо отметить, что LGBM на тесте незначительно улучшила метрику.\n",
    "\n",
    "В целом могу сказать, что обработка тектсов - это самая ресурсоемкая (из того, чем приходилось мне заниматься) сфера ДС. Возможны улучшения показателей при другой обработке признаков (другие библиотеки, другой подход к тэгам и т.д.) Но в виду ресурсоемкости проводить такие эксперименты очень накладно (по крайней мере я бы не стал это делать, просто пробуя разные инструменты- как это делал с табличными данными)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
